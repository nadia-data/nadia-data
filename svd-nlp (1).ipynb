{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:27.451361Z","iopub.execute_input":"2021-06-20T10:37:27.451762Z","iopub.status.idle":"2021-06-20T10:37:28.032531Z","shell.execute_reply.started":"2021-06-20T10:37:27.451729Z","shell.execute_reply":"2021-06-20T10:37:28.031667Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords_en = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:28.034144Z","iopub.execute_input":"2021-06-20T10:37:28.034752Z","iopub.status.idle":"2021-06-20T10:37:28.046549Z","shell.execute_reply.started":"2021-06-20T10:37:28.034704Z","shell.execute_reply":"2021-06-20T10:37:28.045393Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"stopwords_en","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:28.049498Z","iopub.execute_input":"2021-06-20T10:37:28.050252Z","iopub.status.idle":"2021-06-20T10:37:28.062913Z","shell.execute_reply.started":"2021-06-20T10:37:28.050201Z","shell.execute_reply":"2021-06-20T10:37:28.062090Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"},"metadata":{}}]},{"cell_type":"code","source":"from string import punctuation\nprint('From string.punctuation:', punctuation)\n# stopwords_json existe dans https://github.com/6/stopwords-json/blob/master/stopwords-all.json\nstopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\nstopwords_json_en = set(stopwords_json['en'])\nstopwords_nltk_en = set(stopwords_en)\n\nstopwords_punct = set(punctuation)\nstoplist=set.union(stopwords_nltk_en,stopwords_punct,stopwords_json_en)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:28.064530Z","iopub.execute_input":"2021-06-20T10:37:28.064981Z","iopub.status.idle":"2021-06-20T10:37:28.094007Z","shell.execute_reply.started":"2021-06-20T10:37:28.064944Z","shell.execute_reply":"2021-06-20T10:37:28.093181Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"From string.punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","output_type":"stream"}]},{"cell_type":"code","source":"def penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' # if mapping isn't found, fall back to Noun.","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:28.095098Z","iopub.execute_input":"2021-06-20T10:37:28.095559Z","iopub.status.idle":"2021-06-20T10:37:28.114049Z","shell.execute_reply.started":"2021-06-20T10:37:28.095522Z","shell.execute_reply":"2021-06-20T10:37:28.113040Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\n\ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [lemmatizer.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in nltk.pos_tag(word_tokenize(text))]\n\nlemmatize_sent('I was going to tell you anyway')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:28.115165Z","iopub.execute_input":"2021-06-20T10:37:28.115666Z","iopub.status.idle":"2021-06-20T10:37:30.555604Z","shell.execute_reply.started":"2021-06-20T10:37:28.115629Z","shell.execute_reply":"2021-06-20T10:37:30.554473Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['i', 'be', 'go', 'to', 'tell', 'you', 'anyway']"},"metadata":{}}]},{"cell_type":"code","source":"def preprocessing(text):\n    stoplist_gen=set.union(stoplist,{'http'})\n    preproccessed_text=[word for word in lemmatize_sent(text) if word not in stoplist_gen]\n    for word in preproccessed_text:\n        if '//t.co/' in word:\n            preproccessed_text.remove(word)\n    return(preproccessed_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:30.556997Z","iopub.execute_input":"2021-06-20T10:37:30.557281Z","iopub.status.idle":"2021-06-20T10:37:30.565343Z","shell.execute_reply.started":"2021-06-20T10:37:30.557254Z","shell.execute_reply":"2021-06-20T10:37:30.564157Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#  Preprocessing && Vectorizing a Dataset of Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"def preprocess_file(filename):\n    tweets = []\n    labels = []\n    f = open(filename)\n    for line in f:\n        tweet_dict = json.loads(line)\n        #print(tweet_dict)\n        tweets.append(preprocessing(tweet_dict[\"text\"]))\n        labels.append(int(tweet_dict[\"label\"]))\n    return tweets,labels","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:30.566810Z","iopub.execute_input":"2021-06-20T10:37:30.567162Z","iopub.status.idle":"2021-06-20T10:37:30.578154Z","shell.execute_reply.started":"2021-06-20T10:37:30.567114Z","shell.execute_reply":"2021-06-20T10:37:30.577006Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\ntrain_data = preprocess_file('../input/analyse-des-sentiment/training.json')\ntrain_data=pd.DataFrame({\"tweets\":train_data[0],\"labels\":train_data[1]})","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:37:30.580691Z","iopub.execute_input":"2021-06-20T10:37:30.581009Z","iopub.status.idle":"2021-06-20T10:38:04.425647Z","shell.execute_reply.started":"2021-06-20T10:37:30.580977Z","shell.execute_reply":"2021-06-20T10:38:04.424427Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test_data = preprocess_file('../input/analyse-des-sentiment/develop.json')\ntest_data=pd.DataFrame({\"tweets\":test_data[0],\"labels\":test_data[1]})","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:38:04.427391Z","iopub.execute_input":"2021-06-20T10:38:04.427731Z","iopub.status.idle":"2021-06-20T10:38:08.223069Z","shell.execute_reply.started":"2021-06-20T10:38:04.427691Z","shell.execute_reply":"2021-06-20T10:38:08.221987Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef None_Function(doc):\n    return doc\nvectorizer = TfidfVectorizer(\n                             tokenizer=None_Function,\n                             preprocessor=None_Function,\n                             sublinear_tf = True,\n                             use_idf = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:38:08.224435Z","iopub.execute_input":"2021-06-20T10:38:08.224721Z","iopub.status.idle":"2021-06-20T10:38:08.231284Z","shell.execute_reply.started":"2021-06-20T10:38:08.224691Z","shell.execute_reply":"2021-06-20T10:38:08.230341Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_vectors = vectorizer.fit_transform(train_data['tweets'])\ntrain_vectors.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:38:08.232709Z","iopub.execute_input":"2021-06-20T10:38:08.233068Z","iopub.status.idle":"2021-06-20T10:38:09.465225Z","shell.execute_reply.started":"2021-06-20T10:38:08.233033Z","shell.execute_reply":"2021-06-20T10:38:09.464348Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.13034518, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]])"},"metadata":{}}]},{"cell_type":"code","source":"test_vectors = vectorizer.transform(test_data['tweets'])\ntest_vectors.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:38:09.466752Z","iopub.execute_input":"2021-06-20T10:38:09.467091Z","iopub.status.idle":"2021-06-20T10:38:09.574606Z","shell.execute_reply.started":"2021-06-20T10:38:09.467058Z","shell.execute_reply":"2021-06-20T10:38:09.573329Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"train_vectors.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:38:09.576340Z","iopub.execute_input":"2021-06-20T10:38:09.576819Z","iopub.status.idle":"2021-06-20T10:38:09.584052Z","shell.execute_reply.started":"2021-06-20T10:38:09.576772Z","shell.execute_reply":"2021-06-20T10:38:09.582762Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(16805, 29954)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn import decomposition\n\nn_comp = 14977\nsvd = decomposition.TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd.fit(train_vectors)\nprint(svd.explained_variance_ratio_.sum())\n\ntrain_vectors = svd.transform(train_vectors)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T10:39:34.918644Z","iopub.execute_input":"2021-06-20T10:39:34.919120Z"},"trusted":true},"execution_count":null,"outputs":[]}]}