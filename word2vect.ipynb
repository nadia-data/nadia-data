{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.053786Z","iopub.execute_input":"2021-06-25T21:37:01.055199Z","iopub.status.idle":"2021-06-25T21:37:01.060297Z","shell.execute_reply.started":"2021-06-25T21:37:01.055079Z","shell.execute_reply":"2021-06-25T21:37:01.059529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords_en = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.062168Z","iopub.execute_input":"2021-06-25T21:37:01.062533Z","iopub.status.idle":"2021-06-25T21:37:01.082177Z","shell.execute_reply.started":"2021-06-25T21:37:01.0625Z","shell.execute_reply":"2021-06-25T21:37:01.081089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords_en","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.084934Z","iopub.execute_input":"2021-06-25T21:37:01.085256Z","iopub.status.idle":"2021-06-25T21:37:01.100002Z","shell.execute_reply.started":"2021-06-25T21:37:01.085225Z","shell.execute_reply":"2021-06-25T21:37:01.099232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation\nprint('From string.punctuation:', punctuation)\n# stopwords_json existe dans https://github.com/6/stopwords-json/blob/master/stopwords-all.json\nstopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\nstopwords_json_en = set(stopwords_json['en'])\nstopwords_nltk_en = set(stopwords_en)\n\nstopwords_punct = set(punctuation)\nstoplist=set.union(stopwords_nltk_en,stopwords_punct,stopwords_json_en)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.101771Z","iopub.execute_input":"2021-06-25T21:37:01.102371Z","iopub.status.idle":"2021-06-25T21:37:01.14156Z","shell.execute_reply.started":"2021-06-25T21:37:01.102335Z","shell.execute_reply":"2021-06-25T21:37:01.14038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' # if mapping isn't found, fall back to Noun.","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.142959Z","iopub.execute_input":"2021-06-25T21:37:01.143293Z","iopub.status.idle":"2021-06-25T21:37:01.15853Z","shell.execute_reply.started":"2021-06-25T21:37:01.143262Z","shell.execute_reply":"2021-06-25T21:37:01.157416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\n\ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [lemmatizer.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in nltk.pos_tag(word_tokenize(text))]\n\nlemmatize_sent('I was going to tell you anyway')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.162965Z","iopub.execute_input":"2021-06-25T21:37:01.163315Z","iopub.status.idle":"2021-06-25T21:37:01.183368Z","shell.execute_reply.started":"2021-06-25T21:37:01.163282Z","shell.execute_reply":"2021-06-25T21:37:01.182205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(text):\n    stoplist_gen=set.union(stoplist,{'http'})\n    preproccessed_text=[word for word in lemmatize_sent(text) if word not in stoplist_gen]\n    for word in preproccessed_text:\n        if '//t.co/' in word:\n            preproccessed_text.remove(word)\n    return(preproccessed_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.192712Z","iopub.execute_input":"2021-06-25T21:37:01.19326Z","iopub.status.idle":"2021-06-25T21:37:01.200035Z","shell.execute_reply.started":"2021-06-25T21:37:01.193218Z","shell.execute_reply":"2021-06-25T21:37:01.198451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing a Data of Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"def preprocess_file(filename):\n    tweets =[]\n    tweets_cleaned = []\n    labels = []\n    f = open(filename)\n    for line in f:\n        tweet_dict = json.loads(line)\n        #print(tweet_dict)\n        tweets.append(tweet_dict[\"text\"])\n        tweets_cleaned.append(preprocessing(tweet_dict[\"text\"]))\n        labels.append(int(tweet_dict[\"label\"]))\n    return tweets,tweets_cleaned,labels","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.202031Z","iopub.execute_input":"2021-06-25T21:37:01.202347Z","iopub.status.idle":"2021-06-25T21:37:01.213871Z","shell.execute_reply.started":"2021-06-25T21:37:01.202317Z","shell.execute_reply":"2021-06-25T21:37:01.212612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\ntrain_data = preprocess_file('../input/analyse-des-sentiment/training.json')\ntrain_data=pd.DataFrame({\"Tweets\":train_data[0],\"Preproccessed Tweets\":train_data[1],\"labels\":train_data[2]})","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:01.216494Z","iopub.execute_input":"2021-06-25T21:37:01.216949Z","iopub.status.idle":"2021-06-25T21:37:35.030313Z","shell.execute_reply.started":"2021-06-25T21:37:01.216906Z","shell.execute_reply":"2021-06-25T21:37:35.029162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:35.032002Z","iopub.execute_input":"2021-06-25T21:37:35.032332Z","iopub.status.idle":"2021-06-25T21:37:35.052643Z","shell.execute_reply.started":"2021-06-25T21:37:35.032301Z","shell.execute_reply":"2021-06-25T21:37:35.05178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = preprocess_file('../input/analyse-des-sentiment/develop.json')\ntest_data=pd.DataFrame({\"tweets\":test_data[0],\"Preproccessed Tweets\":test_data[1],\"labels\":test_data[2]})","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:35.054105Z","iopub.execute_input":"2021-06-25T21:37:35.054436Z","iopub.status.idle":"2021-06-25T21:37:38.860836Z","shell.execute_reply.started":"2021-06-25T21:37:35.054406Z","shell.execute_reply":"2021-06-25T21:37:38.859659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:38.862131Z","iopub.execute_input":"2021-06-25T21:37:38.862487Z","iopub.status.idle":"2021-06-25T21:37:38.883162Z","shell.execute_reply.started":"2021-06-25T21:37:38.862453Z","shell.execute_reply":"2021-06-25T21:37:38.882281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sampling the unbalanced Train Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample\n\ndf_neg=(train_data[train_data[\"labels\"]==-1]).sample(n=2520,random_state=42)\ndf_neu=(train_data[train_data[\"labels\"]==0]).sample(n=2520,random_state=42)\ndf_pos=(train_data[train_data[\"labels\"]==1]).sample(n=2520,random_state=42)\n\ntrain_data=pd.concat([df_neg,df_neu,df_pos])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:38.884269Z","iopub.execute_input":"2021-06-25T21:37:38.884687Z","iopub.status.idle":"2021-06-25T21:37:38.906122Z","shell.execute_reply.started":"2021-06-25T21:37:38.884655Z","shell.execute_reply":"2021-06-25T21:37:38.9049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neg=test_data[test_data.labels == 1]\nprint (len(neg))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:38.90721Z","iopub.execute_input":"2021-06-25T21:37:38.907638Z","iopub.status.idle":"2021-06-25T21:37:38.91493Z","shell.execute_reply.started":"2021-06-25T21:37:38.907608Z","shell.execute_reply":"2021-06-25T21:37:38.913744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_mapping={1:'Positive',-1:'Negative',0:'Neutral'}\nY=train_data['labels'].map(label_mapping)\nY","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:38.918179Z","iopub.execute_input":"2021-06-25T21:37:38.918587Z","iopub.status.idle":"2021-06-25T21:37:38.935026Z","shell.execute_reply.started":"2021-06-25T21:37:38.91855Z","shell.execute_reply":"2021-06-25T21:37:38.934091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(font_scale=1.4)\nY.value_counts(normalize=True).plot(kind='bar', figsize=(7, 6), rot=0)\nplt.xlabel(\"Sentiment Polarity\", labelpad=14)\nplt.ylabel(\"% of each category in the Dataset\", labelpad=14)\nplt.title(\"Distribution of tweets by polarity \", y=1.02);","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:38.937121Z","iopub.execute_input":"2021-06-25T21:37:38.937444Z","iopub.status.idle":"2021-06-25T21:37:39.111181Z","shell.execute_reply.started":"2021-06-25T21:37:38.937411Z","shell.execute_reply":"2021-06-25T21:37:39.110039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the word2vec model","metadata":{}},{"cell_type":"code","source":"# Importing the built-in logging module\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.112658Z","iopub.execute_input":"2021-06-25T21:37:39.112995Z","iopub.status.idle":"2021-06-25T21:37:39.117516Z","shell.execute_reply.started":"2021-06-25T21:37:39.112965Z","shell.execute_reply":"2021-06-25T21:37:39.116321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences=train_data[\"Preproccessed Tweets\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.119138Z","iopub.execute_input":"2021-06-25T21:37:39.11958Z","iopub.status.idle":"2021-06-25T21:37:39.132471Z","shell.execute_reply.started":"2021-06-25T21:37:39.119531Z","shell.execute_reply":"2021-06-25T21:37:39.13109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.133816Z","iopub.execute_input":"2021-06-25T21:37:39.13413Z","iopub.status.idle":"2021-06-25T21:37:39.145823Z","shell.execute_reply.started":"2021-06-25T21:37:39.134102Z","shell.execute_reply":"2021-06-25T21:37:39.144986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the model and setting values for the various parameters\nnum_features = 700  # Word vector dimensionality\nmin_word_count = 40 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\n\nmodel = word2vec.Word2Vec(min_count=1,\n                     window=10,\n                     vector_size=700,\n                     sample=1e-5, \n                     #alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1,\n                     sg=1)\n\n# To make the model memory efficient\nmodel.init_sims(replace=True)\n\n# Saving the model for later use. Can be loaded using Word2Vec.load()\nmodel_name = \"700features_40minwords_10context\"\nmodel.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.146882Z","iopub.execute_input":"2021-06-25T21:37:39.147327Z","iopub.status.idle":"2021-06-25T21:37:39.169433Z","shell.execute_reply.started":"2021-06-25T21:37:39.147296Z","shell.execute_reply":"2021-06-25T21:37:39.168435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = word2vec.Word2Vec.load(\"./700features_40minwords_10context\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.170701Z","iopub.execute_input":"2021-06-25T21:37:39.17103Z","iopub.status.idle":"2021-06-25T21:37:39.188325Z","shell.execute_reply.started":"2021-06-25T21:37:39.170998Z","shell.execute_reply":"2021-06-25T21:37:39.18744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.build_vocab(sentences, progress_per=100000)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.18943Z","iopub.execute_input":"2021-06-25T21:37:39.18988Z","iopub.status.idle":"2021-06-25T21:37:39.91788Z","shell.execute_reply.started":"2021-06-25T21:37:39.189826Z","shell.execute_reply":"2021-06-25T21:37:39.916981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nombre de tweets utilisés dans notre corpus\ntotal_examples = model.corpus_count\nprint(\"{} tweets ont été utilisés pour créer le vocabulaire de notre corpus.\".format(total_examples))\n\n# Taille du vocabulaire de notre modèle\nprint(\"Le vocabulaire comporte {} mots distincts\".format(len(model.wv.key_to_index)))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.919087Z","iopub.execute_input":"2021-06-25T21:37:39.919527Z","iopub.status.idle":"2021-06-25T21:37:39.925135Z","shell.execute_reply.started":"2021-06-25T21:37:39.919495Z","shell.execute_reply":"2021-06-25T21:37:39.923982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.train(sentences, total_examples=model.corpus_count, epochs=30, report_delay=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:37:39.926362Z","iopub.execute_input":"2021-06-25T21:37:39.926663Z","iopub.status.idle":"2021-06-25T21:38:03.225257Z","shell.execute_reply.started":"2021-06-25T21:37:39.926632Z","shell.execute_reply":"2021-06-25T21:38:03.224349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L= \"man woman girl aunt daughter\".split()\nL","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.22631Z","iopub.execute_input":"2021-06-25T21:38:03.226719Z","iopub.status.idle":"2021-06-25T21:38:03.232357Z","shell.execute_reply.started":"2021-06-25T21:38:03.226688Z","shell.execute_reply":"2021-06-25T21:38:03.23128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Few tests: This will print the odd word among them \nmodel.wv.doesnt_match(L)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.233754Z","iopub.execute_input":"2021-06-25T21:38:03.234132Z","iopub.status.idle":"2021-06-25T21:38:03.289864Z","shell.execute_reply.started":"2021-06-25T21:38:03.234099Z","shell.execute_reply":"2021-06-25T21:38:03.288906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"La= \"want guys Foreigner world\".split()\nLa","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.291051Z","iopub.execute_input":"2021-06-25T21:38:03.291334Z","iopub.status.idle":"2021-06-25T21:38:03.296251Z","shell.execute_reply.started":"2021-06-25T21:38:03.291308Z","shell.execute_reply":"2021-06-25T21:38:03.295491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.doesnt_match(La)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.297406Z","iopub.execute_input":"2021-06-25T21:38:03.297914Z","iopub.status.idle":"2021-06-25T21:38:03.313865Z","shell.execute_reply.started":"2021-06-25T21:38:03.297875Z","shell.execute_reply":"2021-06-25T21:38:03.312898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar(\"man\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.315047Z","iopub.execute_input":"2021-06-25T21:38:03.315343Z","iopub.status.idle":"2021-06-25T21:38:03.339719Z","shell.execute_reply.started":"2021-06-25T21:38:03.315315Z","shell.execute_reply":"2021-06-25T21:38:03.33832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar(\"guy\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.3417Z","iopub.execute_input":"2021-06-25T21:38:03.342219Z","iopub.status.idle":"2021-06-25T21:38:03.354931Z","shell.execute_reply.started":"2021-06-25T21:38:03.342171Z","shell.execute_reply":"2021-06-25T21:38:03.353705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.356914Z","iopub.execute_input":"2021-06-25T21:38:03.357747Z","iopub.status.idle":"2021-06-25T21:38:03.363625Z","shell.execute_reply.started":"2021-06-25T21:38:03.357686Z","shell.execute_reply":"2021-06-25T21:38:03.362062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets=train_data[\"Preproccessed Tweets\"]\ntest_tweets=test_data[\"Preproccessed Tweets\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.36588Z","iopub.execute_input":"2021-06-25T21:38:03.366453Z","iopub.status.idle":"2021-06-25T21:38:03.380016Z","shell.execute_reply.started":"2021-06-25T21:38:03.366397Z","shell.execute_reply":"2021-06-25T21:38:03.378798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Averaging Words Vectors to Create Sentence Embedding\ndef get_mean_vector(word2vec_model, words):\n    # remove out-of-vocabulary words\n    words = [word for word in words if word in model.wv.key_to_index]\n    if len(words) >= 1:\n        return np.mean(word2vec_model[words], axis=0)\n    else:\n        return np.zeros(word2vec_model.vector_size)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.385836Z","iopub.execute_input":"2021-06-25T21:38:03.3867Z","iopub.status.idle":"2021-06-25T21:38:03.394406Z","shell.execute_reply.started":"2021-06-25T21:38:03.386641Z","shell.execute_reply":"2021-06-25T21:38:03.392953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_array=[]\ntest_array=[]\nfor tweet in train_tweets:\n    vec = get_mean_vector(model.wv, tweet)\n    if len(vec) > 0:\n        train_array.append(vec)\n        \nfor tweet in test_tweets:\n    vec = get_mean_vector(model.wv, tweet)\n    if len(vec) > 0:\n        test_array.append(vec)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:03.396965Z","iopub.execute_input":"2021-06-25T21:38:03.397486Z","iopub.status.idle":"2021-06-25T21:38:04.123268Z","shell.execute_reply.started":"2021-06-25T21:38:03.39743Z","shell.execute_reply":"2021-06-25T21:38:04.122377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_array[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:04.124431Z","iopub.execute_input":"2021-06-25T21:38:04.124916Z","iopub.status.idle":"2021-06-25T21:38:04.130014Z","shell.execute_reply.started":"2021-06-25T21:38:04.124864Z","shell.execute_reply":"2021-06-25T21:38:04.129206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"train_tags=train_data['labels']\ntest_tags=test_data['labels']","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:04.131215Z","iopub.execute_input":"2021-06-25T21:38:04.131657Z","iopub.status.idle":"2021-06-25T21:38:04.145573Z","shell.execute_reply.started":"2021-06-25T21:38:04.131617Z","shell.execute_reply":"2021-06-25T21:38:04.144649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tags","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:04.146786Z","iopub.execute_input":"2021-06-25T21:38:04.14726Z","iopub.status.idle":"2021-06-25T21:38:04.163535Z","shell.execute_reply.started":"2021-06-25T21:38:04.147214Z","shell.execute_reply":"2021-06-25T21:38:04.162477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\n\n# Perform classification with SVM, kernel=linear\nclassifier_linear = svm.SVC(kernel='linear') # SVC a linear kernel\n\nclassifier_linear.fit(train_array, train_tags)\n\nprediction_linear = classifier_linear.predict(test_array)\n\nResultat=accuracy_score(test_tags,prediction_linear)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:04.164895Z","iopub.execute_input":"2021-06-25T21:38:04.165256Z","iopub.status.idle":"2021-06-25T21:38:52.090772Z","shell.execute_reply.started":"2021-06-25T21:38:04.165207Z","shell.execute_reply":"2021-06-25T21:38:52.089702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Resultat","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:52.092176Z","iopub.execute_input":"2021-06-25T21:38:52.092499Z","iopub.status.idle":"2021-06-25T21:38:52.098467Z","shell.execute_reply.started":"2021-06-25T21:38:52.092466Z","shell.execute_reply":"2021-06-25T21:38:52.097317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot confusion matrix\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix,classification_report\n\ncf_matrix = confusion_matrix(test_tags,prediction_linear )\n\ncf_matrix\nprint(classification_report(test_tags,prediction_linear))\n\ndf_cm = pd.DataFrame(cf_matrix , index = [i for i in [-1,0,1]],columns = [i for i in [-1,0,1]])\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm/np.sum(df_cm), annot=True, \n            fmt='.2%',cmap=\"Blues\")\nplt.xlabel('Predicted Class')\nplt.ylabel('True Class')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:52.100252Z","iopub.execute_input":"2021-06-25T21:38:52.100644Z","iopub.status.idle":"2021-06-25T21:38:52.457284Z","shell.execute_reply.started":"2021-06-25T21:38:52.100605Z","shell.execute_reply":"2021-06-25T21:38:52.456011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=200,max_features='sqrt',random_state=0)\nrf.fit(train_array, train_tags.ravel())\nY_pred=rf.predict(test_array)\nrf.score(test_array,test_tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:38:52.458677Z","iopub.execute_input":"2021-06-25T21:38:52.459052Z","iopub.status.idle":"2021-06-25T21:39:32.148442Z","shell.execute_reply.started":"2021-06-25T21:38:52.459016Z","shell.execute_reply":"2021-06-25T21:39:32.147293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot confusion matrix\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix,classification_report\n\ncf_matrix = confusion_matrix(test_tags,Y_pred )\n\ncf_matrix\nprint(classification_report(test_tags,Y_pred))\n\ndf_cm = pd.DataFrame(cf_matrix , index = [i for i in [-1,0,1]],columns = [i for i in [-1,0,1]])\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm/np.sum(df_cm), annot=True, \n            fmt='.2%',cmap=\"Blues\")\nplt.xlabel('Predicted Class')\nplt.ylabel('True Class')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T21:39:32.149795Z","iopub.execute_input":"2021-06-25T21:39:32.150106Z","iopub.status.idle":"2021-06-25T21:39:32.457398Z","shell.execute_reply.started":"2021-06-25T21:39:32.150075Z","shell.execute_reply":"2021-06-25T21:39:32.456369Z"},"trusted":true},"execution_count":null,"outputs":[]}]}